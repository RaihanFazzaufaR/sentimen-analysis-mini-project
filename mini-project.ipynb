{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fc8c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  labels     source  \\\n",
      "0  @USER.wood17 knp lo gak berani bersumpah dan b...       1  Instagram   \n",
      "1  haha, somad somad. Muka dekil otak 0% , kok ya...       1  Instagram   \n",
      "2  hahaha, kaum sableng 212 kl berita begini mrk ...       1  Instagram   \n",
      "3  hahaha, makin stress aja  ni umat sableng, dlu...       1  Instagram   \n",
      "4       HIDUP PSI = partai SAMPAH indonesia..... ...       1  Instagram   \n",
      "\n",
      "        dataset  nb_annotators  \n",
      "0  ID_instagram              3  \n",
      "1  ID_instagram              3  \n",
      "2  ID_instagram              3  \n",
      "3  ID_instagram              3  \n",
      "4  ID_instagram              3  \n",
      "labels\n",
      "0    8256\n",
      "1    6050\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/in_hf.csv')\n",
    "print(df.head())\n",
    "print(df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c0498d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  @USER.wood17 knp lo gak berani bersumpah dan b...      1\n",
      "1  haha, somad somad. Muka dekil otak 0% , kok ya...      1\n",
      "2  hahaha, kaum sableng 212 kl berita begini mrk ...      1\n",
      "3  hahaha, makin stress aja  ni umat sableng, dlu...      1\n",
      "4       HIDUP PSI = partai SAMPAH indonesia..... ...      1\n"
     ]
    }
   ],
   "source": [
    "# label_mapping = {\n",
    "#     'Non_HS' : 0,\n",
    "#     'HS': 1\n",
    "# }\n",
    "\n",
    "# df['Label'] = df['labels']\n",
    "# print(df.head())\n",
    "\n",
    "df.rename(columns={'text': 'text', 'labels': 'label'}, inplace=True)\n",
    "df_final = df[['text', 'label']]\n",
    "\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d80a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as _nltk_stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stopwords = set(_nltk_stopwords.words('indonesian'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "df_final['text'] = df_final['text'].apply(clean_text)\n",
    "df_final.to_csv('dataset/in_hf_cleaned.csv', index=False)\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b382600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df_final, test_size=0.2, random_state=42, stratify=df_final['label'])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8dd134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef623f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3274af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87      1652\n",
      "           1       0.85      0.78      0.81      1210\n",
      "\n",
      "    accuracy                           0.85      2862\n",
      "   macro avg       0.85      0.84      0.84      2862\n",
      "weighted avg       0.85      0.85      0.85      2862\n",
      "\n",
      "Accuracy: 0.8469601677148847\n",
      "Precision, Recall, F1-Score: (np.float64(0.8469692472437145), np.float64(0.8469601677148847), np.float64(0.8459349224033756), None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "true_labels = tokenized_test['label']\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predicted_labels))\n",
    "\n",
    "print(\"Precision, Recall, F1-Score:\", precision_recall_fscore_support(true_labels, predicted_labels, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09ab64dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1: [{'label': 'LABEL_0', 'score': 0.9769070744514465}]\n",
      "sentence2: [{'label': 'LABEL_1', 'score': 0.707962691783905}]\n",
      "sentence3: [{'label': 'LABEL_1', 'score': 0.6371983289718628}]\n",
      "sentence4: [{'label': 'LABEL_0', 'score': 0.8528648614883423}]\n",
      "sentence5: [{'label': 'LABEL_1', 'score': 0.7920601963996887}]\n",
      "sentence6: [{'label': 'LABEL_0', 'score': 0.8141269683837891}]\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./final_model\")\n",
    "\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\"./final_model\")\n",
    "final_tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "predictor = pipeline(\"text-classification\", model=final_model, tokenizer=final_tokenizer)\n",
    "\n",
    "sentences_test_1 = \"Saya harap indonesia bisa menjadi lebih baik untuk kedepannya.\"\n",
    "sentences_test_2 = \"Orang-orang dari suku tertentu adalah pembelah bangsa emang babi.\"\n",
    "sentences_test_3 = \"Mereka emang cuman orang kurang berpendidikan, omong doang bisanya, bodoh emang tuh orang.\"\n",
    "sentences_test_4 = \"Memang seperti itu suku jawa, selalu jadi pusat sorotan. Memang bangsat suku itu\"\n",
    "sentences_test_5 = \"boro boro ingin maju, ijazah jelek luar biasa udah sok\"\n",
    "sentences_test_6 = \"Siapa lagi kalau bukan yang dia yang melakukan, hutang tinggi karena siapa kalau bukan dia yang diagungkan\" #sindiran\n",
    "\n",
    "print(f'sentence1: {predictor(sentences_test_1)}')\n",
    "print(f'sentence2: {predictor(sentences_test_2)}')\n",
    "print(f'sentence3: {predictor(sentences_test_3)}')\n",
    "print(f'sentence4: {predictor(sentences_test_4)}')\n",
    "print(f'sentence5: {predictor(sentences_test_5)}')\n",
    "print(f'sentence6: {predictor(sentences_test_6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19c94e",
   "metadata": {},
   "source": [
    "Sindiran tidak terbaca dengan benar (baru mendeteksi ketika kata kata frontal dilontarkan) kemungkinan karena kekurangan data yang memang berbau menyindir. Bagaimana caranya agar dia mendeteksi sindiran juga?\n",
    "\n",
    "Terkadang bila terdapat pujian tetapi diakhiri dengan umpatan tidak dianggap sebagai Hate Speech (Spekulasi saya bisa saja ini tidak benar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
