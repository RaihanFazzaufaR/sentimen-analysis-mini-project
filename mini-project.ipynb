{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8fc8c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  labels     source  \\\n",
      "0  @USER.wood17 knp lo gak berani bersumpah dan b...       1  Instagram   \n",
      "1  haha, somad somad. Muka dekil otak 0% , kok ya...       1  Instagram   \n",
      "2  hahaha, kaum sableng 212 kl berita begini mrk ...       1  Instagram   \n",
      "3  hahaha, makin stress aja  ni umat sableng, dlu...       1  Instagram   \n",
      "4       HIDUP PSI = partai SAMPAH indonesia..... ...       1  Instagram   \n",
      "\n",
      "        dataset  nb_annotators  \n",
      "0  ID_instagram              3  \n",
      "1  ID_instagram              3  \n",
      "2  ID_instagram              3  \n",
      "3  ID_instagram              3  \n",
      "4  ID_instagram              3  \n",
      "labels\n",
      "0    8256\n",
      "1    6050\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/indonesian-hate-speech-superset/in_hf.csv')\n",
    "print(df.head())\n",
    "print(df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0498d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  @USER.wood17 knp lo gak berani bersumpah dan b...      1\n",
      "1  haha, somad somad. Muka dekil otak 0% , kok ya...      1\n",
      "2  hahaha, kaum sableng 212 kl berita begini mrk ...      1\n",
      "3  hahaha, makin stress aja  ni umat sableng, dlu...      1\n",
      "4       HIDUP PSI = partai SAMPAH indonesia..... ...      1\n"
     ]
    }
   ],
   "source": [
    "# label_mapping = {\n",
    "#     'Non_HS' : 0,\n",
    "#     'HS': 1\n",
    "# }\n",
    "\n",
    "# df['Label'] = df['labels']\n",
    "# print(df.head())\n",
    "\n",
    "df.rename(columns={'text': 'text', 'labels': 'label'}, inplace=True)\n",
    "df_final = df[['text', 'label']]\n",
    "\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d80a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  wood17 knp lo gak berani bersumpah bertaruh kr...      1\n",
      "1  haha somad somad muka dekil otak 0 ya g malu n...      1\n",
      "2    hahaha kaum sableng 212 kl berita mrk buta tuli      1\n",
      "3  hahaha stress aja ni umat sableng dlu raja sal...      1\n",
      "4                  hidup psi partai sampah indonesia      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\septina erawati\\AppData\\Local\\Temp\\ipykernel_13744\\2278696936.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['text'] = df_final['text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as _nltk_stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stopwords = set(_nltk_stopwords.words('indonesian'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "df_final['text'] = df_final['text'].apply(clean_text)\n",
    "df_final.to_csv('dataset/in_hf_cleaned.csv', index=False)\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a4e3f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b382600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 11444, Test size: 2862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df_final, test_size=0.2, random_state=42, stratify=df_final['label'])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8dd134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "269d2db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6291302c2c50497eadc221472b41f452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43114f7e58e44173859968b16b7d05d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2862 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e11d2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Programming\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca03eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef623f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2864' max='2864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2864/2864 16:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.656600</td>\n",
       "      <td>0.634098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.478592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.435579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.424521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.445365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.422135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.455300</td>\n",
       "      <td>0.396515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.514377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.392998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.378754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.510028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.481700</td>\n",
       "      <td>0.375983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.377934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.359860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.516231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.421219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.483986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.504558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.454713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.371157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.416763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.344300</td>\n",
       "      <td>0.420077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.386422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.392877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.414359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.287000</td>\n",
       "      <td>0.367508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.363404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.327400</td>\n",
       "      <td>0.373606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.467100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.518547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>0.574447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.585944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.543937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.574293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.075900</td>\n",
       "      <td>0.618825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.517488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.585729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.509765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.138400</td>\n",
       "      <td>0.521060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.582839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.568016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.166100</td>\n",
       "      <td>0.507511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.595931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.633383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.661190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.641923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.725646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>0.634197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.666522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.698266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.714505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.701371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.683720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.701466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.698646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.690618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.681212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2864, training_loss=0.2315329417453652, metrics={'train_runtime': 962.3428, 'train_samples_per_second': 47.567, 'train_steps_per_second': 2.976, 'total_flos': 1186199193098400.0, 'train_loss': 0.2315329417453652, 'epoch': 4.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3274af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      1652\n",
      "           1       0.81      0.82      0.81      1210\n",
      "\n",
      "    accuracy                           0.84      2862\n",
      "   macro avg       0.84      0.84      0.84      2862\n",
      "weighted avg       0.84      0.84      0.84      2862\n",
      "\n",
      "Accuracy: 0.8413696715583509\n",
      "Precision, Recall, F1-Score: (np.float64(0.8416041157241183), np.float64(0.8413696715583509), np.float64(0.8414719096013785), None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "true_labels = tokenized_test['label']\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predicted_labels))\n",
    "\n",
    "print(\"Precision, Recall, F1-Score:\", precision_recall_fscore_support(true_labels, predicted_labels, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09ab64dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9790983200073242}]\n",
      "[{'label': 'LABEL_1', 'score': 0.6681391596794128}]\n",
      "[{'label': 'LABEL_1', 'score': 0.6054683923721313}]\n",
      "[{'label': 'LABEL_1', 'score': 0.5968902111053467}]\n",
      "[{'label': 'LABEL_1', 'score': 0.9608490467071533}]\n",
      "[{'label': 'LABEL_0', 'score': 0.7696002125740051}]\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./final_model\")\n",
    "\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\"./final_model\")\n",
    "final_tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "predictor = pipeline(\"text-classification\", model=final_model, tokenizer=final_tokenizer)\n",
    "\n",
    "sentences_test_1 = \"Saya harap indonesia bisa menjadi lebih baik untuk kedepannya.\"\n",
    "sentences_test_2 = \"Orang-orang dari suku tertentu adalah pembelah bangsa emang.\"\n",
    "sentences_test_3 = \"Mereka emang cuman orang kurang berpendidikan, omong doang bisanya bodoh emang.\"\n",
    "sentences_test_4 = \"Memang seperti itu suku jawa, selalu jadi pusat sorotan. Memang tolol suku itu\"\n",
    "sentences_test_5 = \"boro boro ingin maju, ijazah jelek luar biasa udah sok\"\n",
    "sentences_test_6 = \"Siapa lagi kalau bukan yang dia yang melakukan, hutang tinggi karena siapa kalau bukan dia yang diagungkan\" #sindiran\n",
    "\n",
    "print(predictor(sentences_test_1))\n",
    "print(predictor(sentences_test_2))\n",
    "print(predictor(sentences_test_3))\n",
    "print(predictor(sentences_test_4))\n",
    "print(predictor(sentences_test_5))\n",
    "print(predictor(sentences_test_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19c94e",
   "metadata": {},
   "source": [
    "Sindiran tidak terbaca dengan benar (baru mendeteksi ketika kata kata frontal dilontarkan) kemungkinan karena kekurangan data yang memang berbau menyindir. Bagaimana caranya agar dia mendeteksi sindiran juga?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
